{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "df89053c-55da-4946-b602-c3baf619b53f"
    ]
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is stored in the `/datasets/faces/` folder, there you can find\n",
    "- The `final_files` folder with 7.6k photos\n",
    "- The `labels.csv` file with labels, with two columns: `file_name` and `real_age`\n",
    "\n",
    "Given the fact that the number of image files is rather high, it is advisable to avoid reading them all at once, which would greatly consume computational resources. We recommend you build a generator with the ImageDataGenerator generator. This method was explained in Chapter 3, Lesson 7 of this course.\n",
    "\n",
    "The label file can be loaded as an usual CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'https://practicum-content.s3.us-west-1.amazonaws.com/datasets/faces/labels.csv'\n",
    "# photos_path = 'https://practicum-content.s3.us-west-1.amazonaws.com/datasets/faces/final_files/'\n",
    "\n",
    "\n",
    "labels = pd.read_csv('https://practicum-content.s3.us-west-1.amazonaws.com/datasets/faces/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.info()\n",
    "labels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all file_name items are unique\n",
    "are_filenames_unique = labels['file_name'].is_unique\n",
    "\n",
    "# Print the result\n",
    "print(f\"Are all file names unique? {are_filenames_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(labels.isnull().sum())\n",
    "print(labels['real_age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is complete with no missing values in either the file_name or real_age columns. This is excellent news as it means we can proceed without needing to handle missing data, ensuring that our analysis and model training use the full dataset.\n",
    "\n",
    "Age Distribution:\n",
    "\n",
    "- The dataset contains a total of 7,591 entries, indicating a substantial amount of data for training and evaluating the model.\n",
    "- The mean age is approximately 31.2 years, with a standard deviation of about 17.14 years. This suggests a wide range of ages among the individuals in the dataset.\n",
    "- The age range is from 1 to 100 years old, demonstrating a very diverse set of data in terms of age. This diversity is beneficial for training a model that can accurately predict a wide range of ages.\n",
    "- The 25th percentile is at 20 years, the median (50th percentile) is at 29 years, and the 75th percentile is at 41 years. This indicates that half of the dataset's individuals are between 20 and 41 years old, with a skew towards younger ages.\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "- Data Quality: The high quality of the dataset (no missing values, unique file names) makes it a solid foundation for further analysis and model training.\n",
    "- Age Diversity: The broad age range and standard deviation suggest the dataset captures a wide variety of age groups, which is crucial for developing a model capable of accurately assessing ages across the spectrum.\n",
    "- Model Training Implications: The diversity in age and the distribution skewed slightly towards younger ages might influence how the model is trained, potentially requiring techniques to ensure it does not become biased towards more frequently represented age groups.\n",
    "\n",
    "Given this analysis, it's clear that the dataset is well-prepared for the next steps in the project, including more detailed exploratory data analysis, model training, and evaluation. The broad age range and substantial dataset size are promising for training a robust model capable of accurately determining individuals' ages from photographs, which is essential for the project's goal of adhering to alcohol laws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot the distribution of ages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(labels['real_age'], bins=30, kde=True)\n",
    "plt.title('Distribution of Ages in the Dataset')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age distribution in the dataset is roughly bell-shaped but is skewed to the right, indicating a larger proportion of younger individuals. The most common age range appears to be between approximately 20 and 30 years old. There is a significant decline in frequency as age increases, with very few individuals in the older age range (60+ years). This skewness towards younger ages might suggest that the model trained on this dataset could perform better at estimating the ages of younger individuals compared to older ones, due to more examples to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display sample images from different age groups\n",
    "def display_sample_images(image_paths, n_samples=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        img = load_img(image_path, target_size=(224, 224))\n",
    "        plt.subplot(1, n_samples, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select random sample file names\n",
    "sample_files = labels.sample(n=10)['file_name'].values\n",
    "sample_image_paths = ['/datasets/faces/final_files/' + file_name for file_name in sample_files]\n",
    "\n",
    "# Display the images\n",
    "display_sample_images(sample_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the provided sample images, we can draw the following conclusions:\n",
    "\n",
    "- Variety in Age: The images represent a range of ages from children to older adults. This variety is crucial for training a model that needs to recognize and predict ages across a broad spectrum.\n",
    "- Image Quality: The quality of the images varies. Some images appear to be clear, while others seem to be of lower resolution or have some blur, which could impact the model's ability to extract age-related features.\n",
    "- Lighting Conditions: There is a noticeable variation in lighting conditions across the images. Some faces are well-lit, while others are in shadow or have uneven lighting. Such variations can pose a challenge for age estimation models and may require the use of data augmentation techniques to make the model more robust to different lighting conditions.\n",
    "- Background and Pose: The backgrounds vary from neutral to noisy, and the subjects have different head poses. The diversity in background and pose is beneficial for training a model to focus on facial features rather than background elements. However, extreme poses or occlusions could make age prediction more challenging.\n",
    "- Facial Expressions: The sample shows a variety of facial expressions, which can affect apparent age. Training a model to account for these variations is important for accurate age estimation.\n",
    "- Accessories and Hairstyles: Some individuals are wearing glasses, hats, or have hairstyles that partially obscure their faces. These factors can influence age perception and should be considered when training the model.\n",
    "\n",
    "The sample images reflect the diversity in age, image quality, lighting, background, pose, and facial expressions that we would expect in a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Distribution:\n",
    "\n",
    "- The dataset contains a wide range of ages, from 1 to 100 years old, with a total of 7,591 images.\n",
    "- The mean age is approximately 31.2 years, with the age distribution being right-skewed, indicating a higher concentration of younger individuals, particularly in the 20-30 year age range.\n",
    "- This skewness suggests that there is more data available for younger individuals, which could result in the model being more accurate for these ages due to the larger amount of training data.\n",
    "\n",
    "Image Samples:\n",
    "\n",
    "- A visual inspection of a subset of images reveals a diversity in age, suggesting that the dataset has a broad representation that could be beneficial for training a model to recognize a range of ages.\n",
    "- The sample images also show variability in image quality, lighting conditions, and background noise. These variations are representative of real-world conditions but could pose challenges for the model's performance.\n",
    "- Some images have accessories like glasses and hats, and there are varying facial expressions and head poses, which could affect age perception and need to be considered during model training.\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "- Data Quality and Quantity: The dataset is comprehensive and lacks missing values, providing a strong foundation for model training. However, the skew towards younger ages could lead to biases in the model's performance, which would need to be addressed, potentially through data augmentation or weighted loss functions during training.\n",
    "- Model Training and Validation: Given the variations in image quality and conditions, the model should be robust to such variations, which might involve using a pre-trained network or incorporating data augmentation techniques that simulate different lighting conditions and pose variations.\n",
    "- Potential for Bias: The imbalance in age distribution points to the potential for age prediction bias, which should be a consideration when splitting the dataset into training and validation sets. Stratified sampling could help ensure that the model is validated against an age distribution that mirrors the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the necessary functions to train your model on the GPU platform and build a single script containing all of them along with the initialization section.\n",
    "\n",
    "To make this task easier, you can define them in this notebook and run a ready code in the next section to automatically compose the script.\n",
    "\n",
    "The definitions below will be checked by project reviewers as well, so that they can understand how you built the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    It loads the train part of dataset from path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an instance of the ImageDataGenerator class\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,       # Rescale the image by normalizing pixel values\n",
    "        validation_split=0.2, # Reserve 20% of the data for validation\n",
    "        horizontal_flip=True, # Augment the data by flipping images horizontally\n",
    "        vertical_flip=True,   # Augment the data by flipping images vertically\n",
    "    )\n",
    "    \n",
    "    # Create a generator that will read the training data\n",
    "    train_gen_flow = datagen.flow_from_dataframe(\n",
    "        dataframe=pd.read_csv(path + 'labels.csv'), # Load labels\n",
    "        directory=path + 'final_files/',           # Path to the image files\n",
    "        x_col='file_name',                          # Column in dataframe that contains the filenames\n",
    "        y_col='real_age',                           # Column in dataframe that contains the target\n",
    "        target_size=(224, 224),                     # The dimensions to which all images found will be resized\n",
    "        batch_size=32,                              # Size of the batches of data\n",
    "        class_mode='raw',                           # Determines the type of label arrays that are returned\n",
    "        subset='training',                          # Specifies that this is training data\n",
    "        seed=12345                                  # Random seed for shuffling and transformations\n",
    "    )\n",
    "\n",
    "\n",
    "    return train_gen_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    It loads the validation/test part of dataset from path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an instance of the ImageDataGenerator class\n",
    "    # Here we only rescale the validation data, without augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,       # Rescale the image by normalizing pixel values\n",
    "        validation_split=0.2  # Reserve 20% of the data for validation\n",
    "    )\n",
    "    \n",
    "    # Create a generator that will read the test data\n",
    "    test_gen_flow = datagen.flow_from_dataframe(\n",
    "        dataframe=pd.read_csv(path + 'labels.csv'), # Load labels\n",
    "        directory=path + 'final_files/',           # Path to the image files\n",
    "        x_col='file_name',                          # Column in dataframe that contains the filenames\n",
    "        y_col='real_age',                           # Column in dataframe that contains the target\n",
    "        target_size=(224, 224),                     # The dimensions to which all images found will be resized\n",
    "        batch_size=32,                              # Size of the batches of data\n",
    "        class_mode='raw',                           # Determines the type of label arrays that are returned\n",
    "        subset='validation',                        # Specifies that this is validation data\n",
    "        seed=12345                                  # Random seed for shuffling and transformations\n",
    "    )\n",
    "\n",
    "    return test_gen_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    It defines the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the base model, ResNet50, with weights pre-trained on ImageNet\n",
    "    backbone = ResNet50(input_shape=input_shape,\n",
    "                        weights='imagenet',\n",
    "                        include_top=False)\n",
    "    \n",
    "    # Freeze the layers of the backbone\n",
    "    backbone.trainable = False\n",
    "    \n",
    "    # Define the custom head for our network\n",
    "    model = Sequential([\n",
    "        backbone,\n",
    "        GlobalAveragePooling2D(),      # Add GAP layer to reduce the spatial dimensions\n",
    "        Flatten(),                     # Flatten the output\n",
    "        Dense(256, activation='relu'), # Add a fully connected layer with 256 units\n",
    "        Dropout(0.5),                  # Add dropout for regularization\n",
    "        Dense(1, activation='linear')  # Output layer with a single neuron (for regression)\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(lr=0.0001), # Optimizer\n",
    "                  loss='mean_squared_error',  # Loss function for regression\n",
    "                  metrics=['mae'])           # Metric to monitor\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, batch_size=None, epochs=20,\n",
    "                steps_per_epoch=None, validation_steps=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains the model given the parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if steps_per_epoch is None:\n",
    "        steps_per_epoch = len(train_data)\n",
    "    if validation_steps is None:\n",
    "        validation_steps = len(test_data)\n",
    "        \n",
    "    # Adding learning rate scheduler and early stopping for better training control\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=0.000001)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    callbacks = [lr_scheduler, early_stopping]\n",
    "\n",
    "    model.fit(train_data,\n",
    "              validation_data=test_data,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_steps=validation_steps,\n",
    "              verbose=2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Script to Run on the GPU Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given you've defined the necessary functions you can compose a script for the GPU platform, download it via the \"File|Open...\" menu, and to upload it later for running on the GPU platform.\n",
    "\n",
    "N.B.: The script should include the initialization section as well. An example of this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a script to run on the GPU platform\n",
    "\n",
    "init_str = \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\"\"\"\n",
    "\n",
    "import inspect\n",
    "\n",
    "with open('run_model_on_gpu.py', 'w') as f:\n",
    "    \n",
    "    f.write(init_str)\n",
    "    f.write('\\n\\n')\n",
    "        \n",
    "    for fn_name in [load_train, load_test, create_model, train_model]:\n",
    "        \n",
    "        src = inspect.getsource(fn_name)\n",
    "        f.write(src)\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = '/datasets/faces/' \n",
    "\n",
    "    train_data = load_train(path)\n",
    "    test_data = load_test(path)\n",
    "\n",
    "    model = create_model(input_shape=(150, 150, 3)) \n",
    "\n",
    "    model = train_model(model, train_data, test_data, batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the output from the GPU platform as an Markdown cell here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ]  Notebook was opened\n",
    "- [ ]  The code is error free\n",
    "- [ ]  The cells with code have been arranged by order of execution\n",
    "- [ ]  The exploratory data analysis has been performed\n",
    "- [ ]  The results of the exploratory data analysis are presented in the final notebook\n",
    "- [ ]  The model's MAE score is not higher than 8\n",
    "- [ ]  The model training code has been copied to the final notebook\n",
    "- [ ]  The model training output has been copied to the final notebook\n",
    "- [ ]  The findings have been provided based on the results of the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
