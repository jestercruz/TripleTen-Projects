{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Review**\n","\n","Hi, my name is Dmitry and I will be reviewing your project.\n","  \n","You can find my comments in colored markdown cells:\n","  \n","<div class=\"alert alert-success\">\n","  If everything is done successfully.\n","</div>\n","  \n","<div class=\"alert alert-warning\">\n","  If I have some (optional) suggestions, or questions to think about, or general comments.\n","</div>\n","  \n","<div class=\"alert alert-danger\">\n","  If a section requires some corrections. Work can't be accepted with red comments.\n","</div>\n","  \n","Please don't remove my comments, as it will make further review iterations much harder for me.\n","  \n","Feel free to reply to my comments or ask questions using the following template:\n","  \n","<div class=\"alert alert-info\">\n","  For your comments and questions.\n","</div>\n","  \n","First of all, thank you for turning in the project! You did a great job overall, but there are a couple of problems that need to be fixed before the project is accepted. Let me know if you have questions!"]},{"cell_type":"markdown","metadata":{},"source":["# Beta Bank Customer Retention"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","This project focuses on predicting the customer churn using machine learning techniques. Customer churn refers to when a customer stops doing business with a company. Predicting churn is important for Beta Bank as it can help them identify customers who are likely to churn and take proactive steps to retain them.\n","\n","The dataset used contains information about the bank's customers and whether they exited (churned) or not. The data includes customer information such as credit score, gender, age, geography, etc.\n","\n","The project involves the following steps:\n","- Data is loaded, explored, and preprocessed. This includes handling missing values, converting data types, and dropping unnecessary columns.\n","- The target variable is imbalanced with more customers continuing their business compared to those leaving. Techniques such as upsampling the minority class and downsampling the majority class will be used to address this imbalance.\n","- A Logistic Regression model will be trained on the preprocessed data. The model's performance is evaluated using F1 score and AUC-ROC metrics.\n","- The model is then improved using upsampling and downsampling. The results will be compared before the model is improved vs after the model is improved.\n","\n","The goal of this project is to build a model that can accurately predict customer churn. The insights gained from this project could potentially be used to improve Beta Bank's customer retention strategies."]},{"cell_type":"markdown","metadata":{},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":false},"outputs":[],"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import warnings\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score\n","\n","from sklearn.utils import resample\n","from sklearn.utils import shuffle\n","\n","from sklearn.exceptions import FitFailedWarning\n","warnings.filterwarnings(action='ignore', category=UserWarning)\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 14 columns):\n"," #   Column           Non-Null Count  Dtype  \n","---  ------           --------------  -----  \n"," 0   RowNumber        10000 non-null  int64  \n"," 1   CustomerId       10000 non-null  int64  \n"," 2   Surname          10000 non-null  object \n"," 3   CreditScore      10000 non-null  int64  \n"," 4   Geography        10000 non-null  object \n"," 5   Gender           10000 non-null  object \n"," 6   Age              10000 non-null  int64  \n"," 7   Tenure           9091 non-null   float64\n"," 8   Balance          10000 non-null  float64\n"," 9   NumOfProducts    10000 non-null  int64  \n"," 10  HasCrCard        10000 non-null  int64  \n"," 11  IsActiveMember   10000 non-null  int64  \n"," 12  EstimatedSalary  10000 non-null  float64\n"," 13  Exited           10000 non-null  int64  \n","dtypes: float64(3), int64(8), object(3)\n","memory usage: 1.1+ MB\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>RowNumber</th>\n","      <th>CustomerId</th>\n","      <th>Surname</th>\n","      <th>CreditScore</th>\n","      <th>Geography</th>\n","      <th>Gender</th>\n","      <th>Age</th>\n","      <th>Tenure</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>EstimatedSalary</th>\n","      <th>Exited</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4456</th>\n","      <td>4457</td>\n","      <td>15724428</td>\n","      <td>Abel</td>\n","      <td>544</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>8.0</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>61581.20</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4738</th>\n","      <td>4739</td>\n","      <td>15644361</td>\n","      <td>Hooper</td>\n","      <td>702</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>40</td>\n","      <td>NaN</td>\n","      <td>103549.24</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9712.52</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8160</th>\n","      <td>8161</td>\n","      <td>15576990</td>\n","      <td>Taplin</td>\n","      <td>790</td>\n","      <td>Germany</td>\n","      <td>Female</td>\n","      <td>25</td>\n","      <td>5.0</td>\n","      <td>152885.77</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>58214.79</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>729</th>\n","      <td>730</td>\n","      <td>15612525</td>\n","      <td>Preston</td>\n","      <td>499</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>57</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>131372.38</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2709</th>\n","      <td>2710</td>\n","      <td>15780212</td>\n","      <td>Mao</td>\n","      <td>592</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>37</td>\n","      <td>4.0</td>\n","      <td>212692.97</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>176395.02</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6011</th>\n","      <td>6012</td>\n","      <td>15783007</td>\n","      <td>Parker</td>\n","      <td>520</td>\n","      <td>Germany</td>\n","      <td>Female</td>\n","      <td>45</td>\n","      <td>1.0</td>\n","      <td>123086.39</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>41042.40</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6519</th>\n","      <td>6520</td>\n","      <td>15571869</td>\n","      <td>Lei</td>\n","      <td>669</td>\n","      <td>Germany</td>\n","      <td>Female</td>\n","      <td>50</td>\n","      <td>4.0</td>\n","      <td>112650.89</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>166386.22</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7976</th>\n","      <td>7977</td>\n","      <td>15659656</td>\n","      <td>Pan</td>\n","      <td>849</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>35</td>\n","      <td>4.0</td>\n","      <td>110837.73</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>126419.80</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6591</th>\n","      <td>6592</td>\n","      <td>15692110</td>\n","      <td>Ch'eng</td>\n","      <td>758</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>33</td>\n","      <td>7.0</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>188156.34</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6943</th>\n","      <td>6944</td>\n","      <td>15603741</td>\n","      <td>MacDonnell</td>\n","      <td>719</td>\n","      <td>Spain</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>4.0</td>\n","      <td>128389.12</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>176091.31</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      RowNumber  CustomerId     Surname  CreditScore Geography  Gender  Age  \\\n","4456       4457    15724428        Abel          544    France    Male   40   \n","4738       4739    15644361      Hooper          702    France  Female   40   \n","8160       8161    15576990      Taplin          790   Germany  Female   25   \n","729         730    15612525     Preston          499    France  Female   57   \n","2709       2710    15780212         Mao          592    France    Male   37   \n","6011       6012    15783007      Parker          520   Germany  Female   45   \n","6519       6520    15571869         Lei          669   Germany  Female   50   \n","7976       7977    15659656         Pan          849    France    Male   35   \n","6591       6592    15692110      Ch'eng          758    France  Female   33   \n","6943       6944    15603741  MacDonnell          719     Spain    Male   40   \n","\n","      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n","4456     8.0       0.00              2          1               0   \n","4738     NaN  103549.24              1          0               0   \n","8160     5.0  152885.77              1          1               0   \n","729      1.0       0.00              1          0               0   \n","2709     4.0  212692.97              1          0               0   \n","6011     1.0  123086.39              1          1               1   \n","6519     4.0  112650.89              1          0               0   \n","7976     4.0  110837.73              1          0               0   \n","6591     7.0       0.00              1          1               0   \n","6943     4.0  128389.12              1          1               1   \n","\n","      EstimatedSalary  Exited  \n","4456         61581.20       0  \n","4738          9712.52       1  \n","8160         58214.79       0  \n","729         131372.38       1  \n","2709        176395.02       0  \n","6011         41042.40       1  \n","6519        166386.22       1  \n","7976        126419.80       0  \n","6591        188156.34       0  \n","6943        176091.31       0  "]},"metadata":{},"output_type":"display_data"}],"source":["# Read the data\n","data = pd.read_csv('https://practicum-content.s3.us-west-1.amazonaws.com/datasets/Churn.csv')\n","\n","# Examine the data\n","data.info()\n","display(data.sample(10))"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]}],"source":["# Check for duplicates\n","print(data.duplicated().sum())"]},{"cell_type":"markdown","metadata":{},"source":["There are no duplicate rows, so we can move on."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success\">\n","<b>Reviewer's comment</b>\n","\n","The data was loaded and inspected!\n","\n","</div>"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["RowNumber            0\n","CustomerId           0\n","Surname              0\n","CreditScore          0\n","Geography            0\n","Gender               0\n","Age                  0\n","Tenure             909\n","Balance              0\n","NumOfProducts        0\n","HasCrCard            0\n","IsActiveMember       0\n","EstimatedSalary      0\n","Exited               0\n","dtype: int64\n"]}],"source":["# Check for missing values\n","print(data.isnull().sum())"]},{"cell_type":"markdown","metadata":{},"source":["There are 909 missing values for the 'Tenure' column. Some models will not be able to handle data with missing values. Therefore, we will fill in the missing values for tenure with the median value. We will also change the data type of 'Tenure' to integers if all the values are integers."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success\">\n","<b>Reviewer's comment</b>\n","\n","Alright, that's one way to deal with missing values :)\n","\n","</div>"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 14 columns):\n"," #   Column           Non-Null Count  Dtype  \n","---  ------           --------------  -----  \n"," 0   RowNumber        10000 non-null  int64  \n"," 1   CustomerId       10000 non-null  int64  \n"," 2   Surname          10000 non-null  object \n"," 3   CreditScore      10000 non-null  int64  \n"," 4   Geography        10000 non-null  object \n"," 5   Gender           10000 non-null  object \n"," 6   Age              10000 non-null  int64  \n"," 7   Tenure           10000 non-null  int32  \n"," 8   Balance          10000 non-null  float64\n"," 9   NumOfProducts    10000 non-null  int64  \n"," 10  HasCrCard        10000 non-null  int64  \n"," 11  IsActiveMember   10000 non-null  int64  \n"," 12  EstimatedSalary  10000 non-null  float64\n"," 13  Exited           10000 non-null  int64  \n","dtypes: float64(2), int32(1), int64(8), object(3)\n","memory usage: 1.0+ MB\n","None\n"]}],"source":["# Fill missing values in 'Tenure' with the median value\n","data['Tenure'].fillna(data['Tenure'].median(), inplace=True)\n","\n","# Check to see if it's save to convert 'Tenure' from float to int. If so, then convert it.\n","if np.array_equal(data['Tenure'], data['Tenure'].astype('int')):\n","    data['Tenure'] = data['Tenure'].astype('int')\n","\n","print(data.info())\n"]},{"cell_type":"markdown","metadata":{},"source":["We will now remove the columns that are not needed."]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":false},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditScore</th>\n","      <th>Geography</th>\n","      <th>Gender</th>\n","      <th>Age</th>\n","      <th>Tenure</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>EstimatedSalary</th>\n","      <th>Exited</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9254</th>\n","      <td>686</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>32</td>\n","      <td>6</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>179093.26</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1500</th>\n","      <td>630</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>50</td>\n","      <td>1</td>\n","      <td>81947.76</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>63606.22</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5323</th>\n","      <td>622</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>32</td>\n","      <td>5</td>\n","      <td>179305.09</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>149043.78</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>260</th>\n","      <td>732</td>\n","      <td>Germany</td>\n","      <td>Male</td>\n","      <td>42</td>\n","      <td>9</td>\n","      <td>108748.08</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>65323.11</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9064</th>\n","      <td>521</td>\n","      <td>Germany</td>\n","      <td>Female</td>\n","      <td>49</td>\n","      <td>5</td>\n","      <td>127948.57</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>182765.14</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5586</th>\n","      <td>816</td>\n","      <td>Germany</td>\n","      <td>Female</td>\n","      <td>25</td>\n","      <td>2</td>\n","      <td>150355.35</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>35770.84</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1159</th>\n","      <td>729</td>\n","      <td>Spain</td>\n","      <td>Male</td>\n","      <td>37</td>\n","      <td>10</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>100862.54</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4469</th>\n","      <td>612</td>\n","      <td>Spain</td>\n","      <td>Male</td>\n","      <td>33</td>\n","      <td>5</td>\n","      <td>69478.57</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>8973.67</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8945</th>\n","      <td>542</td>\n","      <td>Spain</td>\n","      <td>Male</td>\n","      <td>35</td>\n","      <td>2</td>\n","      <td>174894.53</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>22314.55</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6263</th>\n","      <td>445</td>\n","      <td>France</td>\n","      <td>Male</td>\n","      <td>37</td>\n","      <td>3</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>180012.39</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n","9254          686    France    Male   32       6       0.00              2   \n","1500          630    France    Male   50       1   81947.76              1   \n","5323          622    France    Male   32       5  179305.09              1   \n","260           732   Germany    Male   42       9  108748.08              2   \n","9064          521   Germany  Female   49       5  127948.57              1   \n","5586          816   Germany  Female   25       2  150355.35              2   \n","1159          729     Spain    Male   37      10       0.00              2   \n","4469          612     Spain    Male   33       5   69478.57              1   \n","8945          542     Spain    Male   35       2  174894.53              1   \n","6263          445    France    Male   37       3       0.00              2   \n","\n","      HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n","9254          1               1        179093.26       0  \n","1500          0               1         63606.22       1  \n","5323          1               1        149043.78       0  \n","260           1               1         65323.11       0  \n","9064          1               1        182765.14       0  \n","5586          1               1         35770.84       0  \n","1159          1               0        100862.54       0  \n","4469          1               0          8973.67       1  \n","8945          1               1         22314.55       0  \n","6263          1               1        180012.39       0  "]},"metadata":{},"output_type":"display_data"}],"source":["# Drop the columns that are not needed for the model\n","data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n","\n","display(data.sample(10))"]},{"cell_type":"markdown","metadata":{},"source":["These columns were dropped since they do not contribute to the model's prediction of customer churn. For RowNumber is an index column that does not provide meaningful information for the model. CustomerId is a unique identifier for each customer. Including this in the model could associate specific outcomes to the individual customer IDs and may not work well with unseen data. Surname is the customer's last name, which will probably not have influence towards their likelihood to churn."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success\">\n","<b>Reviewer's comment</b>\n","\n","Make sense!\n","\n","</div>"]},{"cell_type":"code","execution_count":7,"metadata":{"scrolled":true,"trusted":false},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditScore</th>\n","      <th>Age</th>\n","      <th>Tenure</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>EstimatedSalary</th>\n","      <th>Exited</th>\n","      <th>Geography_Germany</th>\n","      <th>Geography_Spain</th>\n","      <th>Gender_Male</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3685</th>\n","      <td>695</td>\n","      <td>39</td>\n","      <td>5</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>102763.69</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3626</th>\n","      <td>789</td>\n","      <td>37</td>\n","      <td>6</td>\n","      <td>110689.07</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>71121.04</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7772</th>\n","      <td>792</td>\n","      <td>50</td>\n","      <td>4</td>\n","      <td>146710.76</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>16528.40</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4136</th>\n","      <td>651</td>\n","      <td>44</td>\n","      <td>2</td>\n","      <td>0.00</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>102530.35</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>772</td>\n","      <td>26</td>\n","      <td>7</td>\n","      <td>152400.51</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>79414.00</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3713</th>\n","      <td>709</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>112949.71</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>155231.55</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>725</td>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>75888.20</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45613.75</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>87</th>\n","      <td>729</td>\n","      <td>30</td>\n","      <td>9</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>151869.35</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2963</th>\n","      <td>655</td>\n","      <td>51</td>\n","      <td>3</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>15801.02</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>9241</th>\n","      <td>509</td>\n","      <td>35</td>\n","      <td>8</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>67431.28</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n","3685          695   39       5       0.00              2          0   \n","3626          789   37       6  110689.07              1          1   \n","7772          792   50       4  146710.76              1          1   \n","4136          651   44       2       0.00              3          1   \n","246           772   26       7  152400.51              2          1   \n","3713          709   22       0  112949.71              1          0   \n","57            725   19       0   75888.20              1          0   \n","87            729   30       9       0.00              2          1   \n","2963          655   51       3       0.00              2          0   \n","9241          509   35       8       0.00              2          0   \n","\n","      IsActiveMember  EstimatedSalary  Exited  Geography_Germany  \\\n","3685               0        102763.69       0              False   \n","3626               1         71121.04       1               True   \n","7772               0         16528.40       1               True   \n","4136               0        102530.35       1              False   \n","246                0         79414.00       0               True   \n","3713               0        155231.55       0               True   \n","57                 0         45613.75       0               True   \n","87                 0        151869.35       0              False   \n","2963               1         15801.02       0              False   \n","9241               1         67431.28       0              False   \n","\n","      Geography_Spain  Gender_Male  \n","3685             True        False  \n","3626            False        False  \n","7772            False        False  \n","4136            False         True  \n","246             False         True  \n","3713            False         True  \n","57              False         True  \n","87              False         True  \n","2963            False        False  \n","9241            False         True  "]},"metadata":{},"output_type":"display_data"}],"source":["# Convert categorical data into numerical data\n","data = pd.get_dummies(data, drop_first=True)\n","\n","display(data.sample(10))"]},{"cell_type":"markdown","metadata":{},"source":["We have the new dataframe that has the categories placed into separate columns. To avoid the dummy variable trap, the drop_first argument for get_dummies doesn't include a Geography_France column. It is assumed that the geography is France if it is not Germany or Spain. Same with Gender_Male assuming the gender is Female if Gender_Male is false."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success\">\n","<b>Reviewer's comment</b>\n","\n","Categorical features were encoded\n","\n","</div>"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":false},"outputs":[],"source":["# Split the data into features and target\n","# The 'Exited' column is the target, the rest are features\n","features = data.drop('Exited', axis=1)\n","target = data['Exited']\n","\n","# First, split the data into a training set (60% of the data) and a temp set (40%)\n","features_train, features_temp, target_train, target_temp = train_test_split(\n","    features, target, test_size=0.4, random_state=42)\n","\n","# Then, split the temp set into a validation set (50% of the temp)\n","# and a testing set (50% of the temp)\n","# This will result in a 20/20 split of the entire dataset for validation/testing\n","features_valid, features_test, target_valid, target_test = train_test_split(\n","    features_temp, target_temp, test_size=0.5, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\">\n","<b>Reviewer's comment</b>\n","\n","Note that we need three sets here: train, validation and test. Train set to train the models, validation to compare different models and balancing tehchniques as well as tune hyperparameters, and the test set to evaluate the final model\n","\n","</div>"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    7963\n","1    2037\n","Name: count, dtype: int64\n"]}],"source":["# Examine the balance of classes\n","class_counts = target.value_counts()\n","print(class_counts)\n"]},{"cell_type":"markdown","metadata":{},"source":["This code shows the number of customers who stayed with the company vs those who took their business elsewhere. It shows that there are significantly more customers who are loyal customers than those who left."]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Imbalance Ratio: 3.9091801669121256\n"]}],"source":["# Calculate the imbalance ratio\n","imbalance_ratio = class_counts[0] / class_counts[1]\n","print(f'Imbalance Ratio: {imbalance_ratio}')"]},{"cell_type":"markdown","metadata":{},"source":["This shows that there are about 4 times the loyal customers as there are who took their business elsewhere at the time the data was collected."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Universal variables\n","# Separate majority and minority classes\n","features_zeros = features[target == 0]\n","features_ones = features[target == 1]\n","target_zeros = target[target == 0]\n","target_ones = target[target == 1]\n","\n","# Create a dictionary of hyperparameters that will be used\n","thresholds = np.arange(0, 1, 0.05)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success\">\n","<b>Reviewer's comment</b>\n","\n","Class distribution was examined\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["## Logistic Regression"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["hyperparameters_lr = {\n","    'solver': ['newton-cholesky', 'saga', 'liblinear', 'newton-cg', 'lbfgs', 'sag'],\n","    'class_weight': ['balanced', None],\n","    'thresholds': thresholds\n","}"]},{"cell_type":"markdown","metadata":{},"source":["Define a function that will be used for all the logistic regression models."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def logistic_regression_experiment(hyperparameters=hyperparameters_lr, \n","                                   features_train=features_train, \n","                                   target_train=target_train, \n","                                   features_valid=features_valid, \n","                                   target_valid=target_valid, \n","                                   method=None):\n","    results = []\n","\n","    if method == 'upsampling':\n","        # Upsample minority class to match the number of samples in majority class\n","        features_upsampled = pd.concat([features_zeros] + [resample(features_ones, replace=True, n_samples=len(features_zeros), random_state=42)])\n","        target_upsampled = pd.concat([target_zeros] + [resample(target_ones, replace=True, n_samples=len(target_zeros), random_state=42)])\n","        print(target_upsampled.value_counts())\n","    elif method == 'downsampling':\n","        # Downsample the majority class\n","        features_downsampled = pd.concat([resample(features_zeros, replace=False, n_samples=len(features_ones), random_state=42)] + [features_ones])\n","        target_downsampled = pd.concat([resample(target_zeros, replace=False, n_samples=len(target_ones), random_state=42)] + [target_ones])\n","        print(target_downsampled.value_counts())\n","    else:\n","        print(target_train.value_counts())\n","\n","    for solver in hyperparameters['solver']:\n","        model = LogisticRegression(random_state=42, solver=solver)\n","        \n","        if method == 'class_weight':\n","            for class_weight in hyperparameters['class_weight']:\n","                model.class_weight = class_weight\n","                model.fit(features_train, target_train)\n","                predicted_valid = model.predict(features_valid)\n","                probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                accuracy = accuracy_score(target_valid, predicted_valid)\n","                f1 = f1_score(target_valid, predicted_valid)\n","                auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                results.append(['LogisticRegression', solver, 'class_weight', class_weight, accuracy, f1, auc_roc])\n","        elif method == 'upsampling':\n","            # Shuffle the dataset\n","            features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=42)\n","            model.fit(features_upsampled, target_upsampled)\n","            predicted_valid = model.predict(features_valid)\n","            probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","            accuracy = accuracy_score(target_valid, predicted_valid)\n","            f1 = f1_score(target_valid, predicted_valid)\n","            auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","            results.append(['LogisticRegression', solver, 'upsampling', 'N/A', accuracy, f1, auc_roc])\n","        elif method == 'downsampling':\n","\n","            # Shuffle the dataset\n","            features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state=42)\n","            model.fit(features_downsampled, target_downsampled)\n","            predicted_valid = model.predict(features_valid)\n","            probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","            accuracy = accuracy_score(target_valid, predicted_valid)\n","            f1 = f1_score(target_valid, predicted_valid)\n","            auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","            results.append(['LogisticRegression', solver, 'downsampling', 'N/A', accuracy, f1, auc_roc])\n","        elif method == 'threshold':\n","            for threshold in hyperparameters['thresholds']:\n","                model.fit(features_train, target_train)\n","                probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                predicted_valid = probabilities_valid > threshold\n","                accuracy = accuracy_score(target_valid, predicted_valid)\n","                f1 = f1_score(target_valid, predicted_valid)\n","                auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                results.append(['LogisticRegression', solver, 'threshold', threshold, accuracy, f1, auc_roc])\n","        else:\n","            model.fit(features_train, target_train)\n","            predicted_valid = model.predict(features_valid)\n","            probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","            accuracy = accuracy_score(target_valid, predicted_valid)\n","            f1 = f1_score(target_valid, predicted_valid)\n","            auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","            results.append(['LogisticRegression', solver, 'None', 'None', accuracy, f1, auc_roc])\n","\n","    df = pd.DataFrame(results, columns=['model_type', 'solver', 'method', 'parameter', 'accuracy', 'f1_score', 'auc_roc'])\n","    return df.sort_values('f1_score', ascending=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Train the model"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n","model_type    LogisticRegression\n","solver           newton-cholesky\n","method                      None\n","parameter                   None\n","accuracy                   0.817\n","f1_score                0.296154\n","auc_roc                 0.751975\n","Name: 0, dtype: object\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>solver</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>newton-cholesky</td>\n","      <td>0.8170</td>\n","      <td>0.296154</td>\n","      <td>0.751975</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>newton-cg</td>\n","      <td>0.8160</td>\n","      <td>0.266932</td>\n","      <td>0.741215</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>lbfgs</td>\n","      <td>0.8010</td>\n","      <td>0.111607</td>\n","      <td>0.648376</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>liblinear</td>\n","      <td>0.8055</td>\n","      <td>0.044226</td>\n","      <td>0.635962</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>saga</td>\n","      <td>0.8100</td>\n","      <td>0.000000</td>\n","      <td>0.488424</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sag</td>\n","      <td>0.8100</td>\n","      <td>0.000000</td>\n","      <td>0.500179</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            solver  accuracy  f1_score   auc_roc\n","0  newton-cholesky    0.8170  0.296154  0.751975\n","3        newton-cg    0.8160  0.266932  0.741215\n","4            lbfgs    0.8010  0.111607  0.648376\n","2        liblinear    0.8055  0.044226  0.635962\n","1             saga    0.8100  0.000000  0.488424\n","5              sag    0.8100  0.000000  0.500179"]},"metadata":{},"output_type":"display_data"}],"source":["df_sorted = logistic_regression_experiment(method=None)\n","\n","df_display = df_sorted[['solver', 'accuracy', 'f1_score', 'auc_roc']]\n","display(df_display)\n","best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["Here's what can be seen from the results:\n","- The precision, which is the ratio of correctly predicted positive observations to the total predicted positives vs false positives, is high with for 0, but relatively low for 1. The precision is .81 for 0 and .45 for 1.\n","- Recall is the ratio of correctly predicted positive observations to all the observations in the class. For 0, the recall is .98, while for 1, the recall is .07.\n","- F1 score is the weighted average of Precision and Recall. This score takes both false positive and false negatives into account. It is a better measure than accuracy for uneven class distribution such as what we have in our data. The F1 score for 0 is .89, while the F1 score for 1 is 0.12.\n","- Support is the number of actual occurences of the class specified in the dataset. For 0, it is 1607 and 1 is 393.\n","\n","From these metrics, we can conclude that the model is performing well in predicting customers who did not exit (0), but not as well as predicting customers who exited."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\">\n","<b>Reviewer's comment</b>\n","\n","Great, you trained a model without taking the imbalance into account first. Note that you need to use the validation set to evaluate the model here: the test set should only be used once you've selected the model and are not going to make any changes. The same goes for models below\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["### Improve the model"]},{"cell_type":"markdown","metadata":{},"source":["#### Explore Class Weight Adjustment\n","\n","Class Weight Adjustment is useful on data that has imbalanced classes, such as what we currently have."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>solver</th>\n","      <th>parameter</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>newton-cholesky</td>\n","      <td>balanced</td>\n","      <td>0.461404</td>\n","      <td>0.752815</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>newton-cg</td>\n","      <td>balanced</td>\n","      <td>0.460733</td>\n","      <td>0.752565</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>liblinear</td>\n","      <td>balanced</td>\n","      <td>0.457483</td>\n","      <td>0.748315</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>lbfgs</td>\n","      <td>balanced</td>\n","      <td>0.394758</td>\n","      <td>0.678908</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>sag</td>\n","      <td>balanced</td>\n","      <td>0.330759</td>\n","      <td>0.548354</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>saga</td>\n","      <td>balanced</td>\n","      <td>0.329276</td>\n","      <td>0.546681</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>newton-cholesky</td>\n","      <td>None</td>\n","      <td>0.296154</td>\n","      <td>0.751975</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>newton-cg</td>\n","      <td>None</td>\n","      <td>0.266932</td>\n","      <td>0.741215</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>lbfgs</td>\n","      <td>None</td>\n","      <td>0.111607</td>\n","      <td>0.648376</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>liblinear</td>\n","      <td>None</td>\n","      <td>0.044226</td>\n","      <td>0.635962</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>saga</td>\n","      <td>None</td>\n","      <td>0.000000</td>\n","      <td>0.488424</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>sag</td>\n","      <td>None</td>\n","      <td>0.000000</td>\n","      <td>0.500179</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             solver parameter  f1_score   auc_roc\n","0   newton-cholesky  balanced  0.461404  0.752815\n","6         newton-cg  balanced  0.460733  0.752565\n","4         liblinear  balanced  0.457483  0.748315\n","8             lbfgs  balanced  0.394758  0.678908\n","10              sag  balanced  0.330759  0.548354\n","2              saga  balanced  0.329276  0.546681\n","1   newton-cholesky      None  0.296154  0.751975\n","7         newton-cg      None  0.266932  0.741215\n","9             lbfgs      None  0.111607  0.648376\n","5         liblinear      None  0.044226  0.635962\n","3              saga      None  0.000000  0.488424\n","11              sag      None  0.000000  0.500179"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["solver       newton-cholesky\n","parameter           balanced\n","f1_score            0.461404\n","auc_roc             0.752815\n","Name: 0, dtype: object\n"]}],"source":["df_sorted = logistic_regression_experiment(method='class_weight')\n","df_display = df_sorted[['solver', 'parameter', 'f1_score', 'auc_roc']]\n","display(df_display)\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["#### Upsampling"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    7963\n","1    7963\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>solver</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>newton-cholesky</td>\n","      <td>0.7000</td>\n","      <td>0.467140</td>\n","      <td>0.756511</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>newton-cg</td>\n","      <td>0.6985</td>\n","      <td>0.465899</td>\n","      <td>0.756449</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>liblinear</td>\n","      <td>0.6495</td>\n","      <td>0.409436</td>\n","      <td>0.684428</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>lbfgs</td>\n","      <td>0.6395</td>\n","      <td>0.400665</td>\n","      <td>0.680967</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sag</td>\n","      <td>0.4930</td>\n","      <td>0.335518</td>\n","      <td>0.555606</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>saga</td>\n","      <td>0.4815</td>\n","      <td>0.332260</td>\n","      <td>0.550694</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            solver  accuracy  f1_score   auc_roc\n","0  newton-cholesky    0.7000  0.467140  0.756511\n","3        newton-cg    0.6985  0.465899  0.756449\n","2        liblinear    0.6495  0.409436  0.684428\n","4            lbfgs    0.6395  0.400665  0.680967\n","5              sag    0.4930  0.335518  0.555606\n","1             saga    0.4815  0.332260  0.550694"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["solver      newton-cholesky\n","accuracy                0.7\n","f1_score            0.46714\n","auc_roc            0.756511\n","Name: 0, dtype: object\n"]}],"source":["df_sorted = logistic_regression_experiment(method='upsampling')\n","df_display = df_sorted[['solver', 'accuracy', 'f1_score', 'auc_roc']]\n","display(df_display)\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["#### Downsampling"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    2037\n","1    2037\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>solver</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>newton-cholesky</td>\n","      <td>0.6915</td>\n","      <td>0.458297</td>\n","      <td>0.756046</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>newton-cg</td>\n","      <td>0.6900</td>\n","      <td>0.456140</td>\n","      <td>0.755963</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>liblinear</td>\n","      <td>0.6485</td>\n","      <td>0.405748</td>\n","      <td>0.683395</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>lbfgs</td>\n","      <td>0.6370</td>\n","      <td>0.396007</td>\n","      <td>0.680088</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sag</td>\n","      <td>0.4785</td>\n","      <td>0.330122</td>\n","      <td>0.547443</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>saga</td>\n","      <td>0.4780</td>\n","      <td>0.329049</td>\n","      <td>0.546152</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            solver  accuracy  f1_score   auc_roc\n","0  newton-cholesky    0.6915  0.458297  0.756046\n","3        newton-cg    0.6900  0.456140  0.755963\n","2        liblinear    0.6485  0.405748  0.683395\n","4            lbfgs    0.6370  0.396007  0.680088\n","5              sag    0.4785  0.330122  0.547443\n","1             saga    0.4780  0.329049  0.546152"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["solver      newton-cholesky\n","accuracy             0.6915\n","f1_score           0.458297\n","auc_roc            0.756046\n","Name: 0, dtype: object\n"]}],"source":["df_sorted = logistic_regression_experiment(method='downsampling')\n","df_display = df_sorted[['solver', 'accuracy', 'f1_score', 'auc_roc']]\n","display(df_display)\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["#### Threshold Adjustment"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model_type</th>\n","      <th>solver</th>\n","      <th>method</th>\n","      <th>parameter</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cholesky</td>\n","      <td>threshold</td>\n","      <td>0.25</td>\n","      <td>0.7405</td>\n","      <td>0.462176</td>\n","      <td>0.751975</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cholesky</td>\n","      <td>threshold</td>\n","      <td>0.30</td>\n","      <td>0.7785</td>\n","      <td>0.457772</td>\n","      <td>0.751975</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cholesky</td>\n","      <td>threshold</td>\n","      <td>0.20</td>\n","      <td>0.6800</td>\n","      <td>0.457627</td>\n","      <td>0.751975</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cg</td>\n","      <td>threshold</td>\n","      <td>0.25</td>\n","      <td>0.7230</td>\n","      <td>0.456863</td>\n","      <td>0.741215</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cg</td>\n","      <td>threshold</td>\n","      <td>0.20</td>\n","      <td>0.6655</td>\n","      <td>0.452984</td>\n","      <td>0.741215</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>LogisticRegression</td>\n","      <td>saga</td>\n","      <td>threshold</td>\n","      <td>0.90</td>\n","      <td>0.8100</td>\n","      <td>0.000000</td>\n","      <td>0.488424</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cg</td>\n","      <td>threshold</td>\n","      <td>0.95</td>\n","      <td>0.8100</td>\n","      <td>0.000000</td>\n","      <td>0.741215</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cholesky</td>\n","      <td>threshold</td>\n","      <td>0.95</td>\n","      <td>0.8100</td>\n","      <td>0.000000</td>\n","      <td>0.751975</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>LogisticRegression</td>\n","      <td>newton-cholesky</td>\n","      <td>threshold</td>\n","      <td>0.90</td>\n","      <td>0.8100</td>\n","      <td>0.000000</td>\n","      <td>0.751975</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>LogisticRegression</td>\n","      <td>sag</td>\n","      <td>threshold</td>\n","      <td>0.95</td>\n","      <td>0.8100</td>\n","      <td>0.000000</td>\n","      <td>0.500179</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>120 rows × 7 columns</p>\n","</div>"],"text/plain":["             model_type           solver     method  parameter  accuracy  \\\n","5    LogisticRegression  newton-cholesky  threshold       0.25    0.7405   \n","6    LogisticRegression  newton-cholesky  threshold       0.30    0.7785   \n","4    LogisticRegression  newton-cholesky  threshold       0.20    0.6800   \n","65   LogisticRegression        newton-cg  threshold       0.25    0.7230   \n","64   LogisticRegression        newton-cg  threshold       0.20    0.6655   \n","..                  ...              ...        ...        ...       ...   \n","38   LogisticRegression             saga  threshold       0.90    0.8100   \n","79   LogisticRegression        newton-cg  threshold       0.95    0.8100   \n","19   LogisticRegression  newton-cholesky  threshold       0.95    0.8100   \n","18   LogisticRegression  newton-cholesky  threshold       0.90    0.8100   \n","119  LogisticRegression              sag  threshold       0.95    0.8100   \n","\n","     f1_score   auc_roc  \n","5    0.462176  0.751975  \n","6    0.457772  0.751975  \n","4    0.457627  0.751975  \n","65   0.456863  0.741215  \n","64   0.452984  0.741215  \n","..        ...       ...  \n","38   0.000000  0.488424  \n","79   0.000000  0.741215  \n","19   0.000000  0.751975  \n","18   0.000000  0.751975  \n","119  0.000000  0.500179  \n","\n","[120 rows x 7 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["model_type    LogisticRegression\n","solver           newton-cholesky\n","method                 threshold\n","parameter                   0.25\n","accuracy                  0.7405\n","f1_score                0.462176\n","auc_roc                 0.751975\n","Name: 5, dtype: object\n"]}],"source":["df_sorted = logistic_regression_experiment(method='threshold')\n","df_display = df_sorted\n","display(df_display)\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["## Decision Tree"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["hyperparameters_dt = {\n","    'max_depth': [None, 5, 10],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 5],\n","    'class_weight': ['balanced', None],\n","    'thresholds': thresholds\n","}"]},{"cell_type":"markdown","metadata":{},"source":["Define a function that will be used for all the decision tree models."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Create a function that will be used for all Decision Tree models\n","def decision_tree_experiment(hyperparameters=hyperparameters_dt, \n","                             features_train=features_train, \n","                             target_train=target_train, \n","                             features_valid=features_valid, \n","                             target_valid=target_valid, \n","                             method=None):\n","    results = []\n","\n","    if method == 'upsampling':\n","        # Upsample minority class to match the number of samples in majority class\n","        features_upsampled = pd.concat([features_zeros] + [resample(features_ones, replace=True, n_samples=len(features_zeros), random_state=42)])\n","        target_upsampled = pd.concat([target_zeros] + [resample(target_ones, replace=True, n_samples=len(target_zeros), random_state=42)])\n","        print(target_upsampled.value_counts())\n","    elif method == 'downsampling':\n","        # Downsample the majority class\n","        features_downsampled = pd.concat([resample(features_zeros, replace=False, n_samples=len(features_ones), random_state=42)] + [features_ones])\n","        target_downsampled = pd.concat([resample(target_zeros, replace=False, n_samples=len(target_ones), random_state=42)] + [target_ones])\n","        print(target_downsampled.value_counts())\n","    else:\n","        print(target_train.value_counts())\n","\n","    for max_depth in hyperparameters['max_depth']:\n","        for min_samples_split in hyperparameters['min_samples_split']:\n","            for min_samples_leaf in hyperparameters['min_samples_leaf']:\n","                model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=42)\n","                \n","                if method == 'class_weight':\n","                    for class_weight in hyperparameters['class_weight']:\n","                        model.class_weight = class_weight\n","                        model.fit(features_train, target_train)\n","                        predicted_valid = model.predict(features_valid)\n","                        probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                        accuracy = accuracy_score(target_valid, predicted_valid)\n","                        f1 = f1_score(target_valid, predicted_valid)\n","                        auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                        results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'class_weight', class_weight, accuracy, f1, auc_roc])\n","                elif method == 'upsampling':\n","                    # Shuffle the dataset\n","                    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=42)\n","                    model.fit(features_upsampled, target_upsampled)\n","                    predicted_valid = model.predict(features_valid)\n","                    probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                    accuracy = accuracy_score(target_valid, predicted_valid)\n","                    f1 = f1_score(target_valid, predicted_valid)\n","                    auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                    results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'upsampling', 'N/A', accuracy, f1, auc_roc])\n","                elif method == 'downsampling':\n","                    # Shuffle the dataset\n","                    features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state=42)\n","                    model.fit(features_downsampled, target_downsampled)\n","                    predicted_valid = model.predict(features_valid)\n","                    probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                    accuracy = accuracy_score(target_valid, predicted_valid)\n","                    f1 = f1_score(target_valid, predicted_valid)\n","                    auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                    results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'downsampling', 'N/A', accuracy, f1, auc_roc])\n","                elif method == 'threshold':\n","                    for threshold in hyperparameters['thresholds']:\n","                        model.fit(features_train, target_train)\n","                        probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                        predicted_valid = probabilities_valid > threshold\n","                        accuracy = accuracy_score(target_valid, predicted_valid)\n","                        f1 = f1_score(target_valid, predicted_valid)\n","                        auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                        results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'threshold', threshold, accuracy, f1, auc_roc])\n","                else:\n","                    model.fit(features_train, target_train)\n","                    predicted_valid = model.predict(features_valid)\n","                    probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                    accuracy = accuracy_score(target_valid, predicted_valid)\n","                    f1 = f1_score(target_valid, predicted_valid)\n","                    auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                    results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'None', 'N/A', accuracy, f1, auc_roc])\n","\n","    df = pd.DataFrame(results, columns=['model_type', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'method', 'parameter', 'accuracy', 'f1_score', 'auc_roc'])\n","    return df.sort_values('f1_score', ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Train the Model"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n","Best so far: max_depth                 NaN\n","min_samples_split    2.000000\n","min_samples_leaf     1.000000\n","accuracy             0.826000\n","f1_score             0.685921\n","auc_roc              0.892593\n","Name: 0, dtype: float64\n","model_type           DecisionTree\n","max_depth                     5.0\n","min_samples_split              10\n","min_samples_leaf                5\n","method                       None\n","parameter                     N/A\n","accuracy                    0.848\n","f1_score                 0.511254\n","auc_roc                  0.814623\n","Name: 17, dtype: object\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17</th>\n","      <td>5.0</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>0.511254</td>\n","      <td>0.814623</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>0.511254</td>\n","      <td>0.814623</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>5.0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0.511254</td>\n","      <td>0.814623</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>5.0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.508091</td>\n","      <td>0.805005</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>5.0</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0.508091</td>\n","      <td>0.798223</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    max_depth  min_samples_split  min_samples_leaf  f1_score   auc_roc\n","17        5.0                 10                 5  0.511254  0.814623\n","11        5.0                  2                 5  0.511254  0.814623\n","14        5.0                  5                 5  0.511254  0.814623\n","13        5.0                  5                 2  0.508091  0.805005\n","15        5.0                 10                 1  0.508091  0.798223"]},"metadata":{},"output_type":"display_data"}],"source":["df_sorted = decision_tree_experiment(method=None)\n","\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'f1_score', 'auc_roc']]\n","print(df_sorted.iloc[0])\n","display(df_display.head())\n","\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["### Improve the model"]},{"cell_type":"markdown","metadata":{},"source":["#### Class Weight Adjustment"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>parameter</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>34</th>\n","      <td>5.0</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>balanced</td>\n","      <td>0.526126</td>\n","      <td>0.822382</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>5.0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>balanced</td>\n","      <td>0.526126</td>\n","      <td>0.822382</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>5.0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>balanced</td>\n","      <td>0.526126</td>\n","      <td>0.822382</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>5.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>balanced</td>\n","      <td>0.518919</td>\n","      <td>0.810549</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>5.0</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>balanced</td>\n","      <td>0.518919</td>\n","      <td>0.810549</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    max_depth  min_samples_split  min_samples_leaf parameter  f1_score  \\\n","34        5.0                 10                 5  balanced  0.526126   \n","28        5.0                  5                 5  balanced  0.526126   \n","22        5.0                  2                 5  balanced  0.526126   \n","20        5.0                  2                 2  balanced  0.518919   \n","32        5.0                 10                 2  balanced  0.518919   \n","\n","     auc_roc  \n","34  0.822382  \n","28  0.822382  \n","22  0.822382  \n","20  0.810549  \n","32  0.810549  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth                 5.0\n","min_samples_split          10\n","min_samples_leaf            5\n","parameter            balanced\n","f1_score             0.526126\n","auc_roc              0.822382\n","Name: 34, dtype: object\n"]}],"source":["df_sorted = decision_tree_experiment(method='class_weight')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'parameter', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["#### Upsampling"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    7963\n","1    7963\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.9965</td>\n","      <td>0.990704</td>\n","      <td>0.990789</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.9895</td>\n","      <td>0.972259</td>\n","      <td>0.990463</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.9710</td>\n","      <td>0.926209</td>\n","      <td>0.989835</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.9710</td>\n","      <td>0.925641</td>\n","      <td>0.990930</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0.9590</td>\n","      <td>0.896465</td>\n","      <td>0.987381</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   max_depth  min_samples_split  min_samples_leaf  accuracy  f1_score  \\\n","0        NaN                  2                 1    0.9965  0.990704   \n","3        NaN                  5                 1    0.9895  0.972259   \n","1        NaN                  2                 2    0.9710  0.926209   \n","4        NaN                  5                 2    0.9710  0.925641   \n","6        NaN                 10                 1    0.9590  0.896465   \n","\n","    auc_roc  \n","0  0.990789  \n","3  0.990463  \n","1  0.989835  \n","4  0.990930  \n","6  0.987381  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth                 NaN\n","min_samples_split    2.000000\n","min_samples_leaf     1.000000\n","accuracy             0.996500\n","f1_score             0.990704\n","auc_roc              0.990789\n","Name: 0, dtype: float64\n"]}],"source":["df_sorted = decision_tree_experiment(method='upsampling')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'accuracy', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["#### Downsampling"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    2037\n","1    2037\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.8260</td>\n","      <td>0.685921</td>\n","      <td>0.892593</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.8245</td>\n","      <td>0.664756</td>\n","      <td>0.900431</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.8235</td>\n","      <td>0.662201</td>\n","      <td>0.894950</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.8130</td>\n","      <td>0.661844</td>\n","      <td>0.893648</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0.8135</td>\n","      <td>0.646445</td>\n","      <td>0.900669</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   max_depth  min_samples_split  min_samples_leaf  accuracy  f1_score  \\\n","0        NaN                  2                 1    0.8260  0.685921   \n","4        NaN                  5                 2    0.8245  0.664756   \n","1        NaN                  2                 2    0.8235  0.662201   \n","3        NaN                  5                 1    0.8130  0.661844   \n","6        NaN                 10                 1    0.8135  0.646445   \n","\n","    auc_roc  \n","0  0.892593  \n","4  0.900431  \n","1  0.894950  \n","3  0.893648  \n","6  0.900669  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["0.6859205776173286\n"]}],"source":["df_sorted = decision_tree_experiment(method='downsampling')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'accuracy', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["#### Threshold Adjustment"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>parameter</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>226</th>\n","      <td>5.0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>0.30</td>\n","      <td>0.566248</td>\n","      <td>0.814623</td>\n","    </tr>\n","    <tr>\n","      <th>286</th>\n","      <td>5.0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0.30</td>\n","      <td>0.566248</td>\n","      <td>0.814623</td>\n","    </tr>\n","    <tr>\n","      <th>346</th>\n","      <td>5.0</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>0.30</td>\n","      <td>0.566248</td>\n","      <td>0.814623</td>\n","    </tr>\n","    <tr>\n","      <th>287</th>\n","      <td>5.0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0.35</td>\n","      <td>0.565035</td>\n","      <td>0.814623</td>\n","    </tr>\n","    <tr>\n","      <th>347</th>\n","      <td>5.0</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>0.35</td>\n","      <td>0.565035</td>\n","      <td>0.814623</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     max_depth  min_samples_split  min_samples_leaf  parameter  f1_score  \\\n","226        5.0                  2                 5       0.30  0.566248   \n","286        5.0                  5                 5       0.30  0.566248   \n","346        5.0                 10                 5       0.30  0.566248   \n","287        5.0                  5                 5       0.35  0.565035   \n","347        5.0                 10                 5       0.35  0.565035   \n","\n","      auc_roc  \n","226  0.814623  \n","286  0.814623  \n","346  0.814623  \n","287  0.814623  \n","347  0.814623  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth            5.000000\n","min_samples_split    2.000000\n","min_samples_leaf     5.000000\n","parameter            0.300000\n","f1_score             0.566248\n","auc_roc              0.814623\n","Name: 226, dtype: float64\n"]}],"source":["df_sorted = decision_tree_experiment(method='threshold')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'parameter', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["## Random Forest"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["hyperparameters_rf = {\n","    'n_estimators': [10, 50, 100],\n","    'max_depth': [None, 5, 10],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 5],\n","    'class_weight': ['balanced', None],\n","    'thresholds': thresholds\n","}"]},{"cell_type":"markdown","metadata":{},"source":["Define a function that will be used for all the random forest models."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Create a function that will be used for all Decision Tree models\n","def random_forest_experiment(hyperparameters=hyperparameters_rf, \n","                             features_train=features_train, \n","                             target_train=target_train, \n","                             features_valid=features_valid, \n","                             target_valid=target_valid, \n","                             method=None):\n","    results = []\n","\n","    if method == 'upsampling':\n","        # Upsample minority class to match the number of samples in majority class\n","        features_upsampled = pd.concat([features_zeros] + [resample(features_ones, replace=True, n_samples=len(features_zeros), random_state=42)])\n","        target_upsampled = pd.concat([target_zeros] + [resample(target_ones, replace=True, n_samples=len(target_zeros), random_state=42)])\n","        print(target_upsampled.value_counts())\n","    elif method == 'downsampling':\n","        # Downsample the majority class\n","        features_downsampled = pd.concat([resample(features_zeros, replace=False, n_samples=len(features_ones), random_state=42)] + [features_ones])\n","        target_downsampled = pd.concat([resample(target_zeros, replace=False, n_samples=len(target_ones), random_state=42)] + [target_ones])\n","        print(target_downsampled.value_counts())\n","    else:\n","        print(target_train.value_counts())\n","\n","    for n_estimators in hyperparameters['n_estimators']:\n","        for max_depth in hyperparameters['max_depth']:\n","            for min_samples_split in hyperparameters['min_samples_split']:\n","                for min_samples_leaf in hyperparameters['min_samples_leaf']:\n","                    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=42)\n","                    if method == 'class_weight':\n","                        for class_weight in hyperparameters['class_weight']:\n","                            model.class_weight = class_weight\n","                            model.fit(features_train, target_train)\n","                            predicted_valid = model.predict(features_valid)\n","                            probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                            accuracy = accuracy_score(target_valid, predicted_valid)\n","                            f1 = f1_score(target_valid, predicted_valid)\n","                            auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                            results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'class_weight', class_weight, accuracy, f1, auc_roc])\n","                    elif method == 'upsampling':\n","                        # Shuffle the dataset\n","                        features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=42)\n","                        model.fit(features_upsampled, target_upsampled)\n","                        predicted_valid = model.predict(features_valid)\n","                        probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                        accuracy = accuracy_score(target_valid, predicted_valid)\n","                        f1 = f1_score(target_valid, predicted_valid)\n","                        auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                        results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'upsampling', 'N/A', accuracy, f1, auc_roc])\n","                    elif method == 'downsampling':\n","                        # Shuffle the dataset\n","                        features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state=42)\n","                        model.fit(features_downsampled, target_downsampled)\n","                        predicted_valid = model.predict(features_valid)\n","                        probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                        accuracy = accuracy_score(target_valid, predicted_valid)\n","                        f1 = f1_score(target_valid, predicted_valid)\n","                        auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                        results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'downsampling', 'N/A', accuracy, f1, auc_roc])\n","                    elif method == 'threshold':\n","                        for threshold in hyperparameters['thresholds']:\n","                            model.fit(features_train, target_train)\n","                            probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                            predicted_valid = probabilities_valid > threshold\n","                            accuracy = accuracy_score(target_valid, predicted_valid)\n","                            f1 = f1_score(target_valid, predicted_valid)\n","                            auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                            results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'threshold', threshold, accuracy, f1, auc_roc])\n","                    else:\n","                        model.fit(features_train, target_train)\n","                        predicted_valid = model.predict(features_valid)\n","                        probabilities_valid = model.predict_proba(features_valid)[:, 1]\n","                        accuracy = accuracy_score(target_valid, predicted_valid)\n","                        f1 = f1_score(target_valid, predicted_valid)\n","                        auc_roc = roc_auc_score(target_valid, probabilities_valid)\n","                        results.append(['DecisionTree', max_depth, min_samples_split, min_samples_leaf, 'None', 'N/A', accuracy, f1, auc_roc])\n","\n","    df = pd.DataFrame(results, columns=['model_type', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'method', 'parameter', 'accuracy', 'f1_score', 'auc_roc'])\n","    return df.sort_values('f1_score', ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Train the Model"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>30</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.560261</td>\n","      <td>0.829896</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>10.0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.559322</td>\n","      <td>0.844311</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.555730</td>\n","      <td>0.811037</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>0.555556</td>\n","      <td>0.834056</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.555372</td>\n","      <td>0.834625</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    max_depth  min_samples_split  min_samples_leaf  f1_score   auc_roc\n","30        NaN                  5                 1  0.560261  0.829896\n","45       10.0                  2                 1  0.559322  0.844311\n","3         NaN                  5                 1  0.555730  0.811037\n","34        NaN                 10                 2  0.555556  0.834056\n","57        NaN                  5                 1  0.555372  0.834625"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth                 NaN\n","min_samples_split    5.000000\n","min_samples_leaf     1.000000\n","f1_score             0.560261\n","auc_roc              0.829896\n","Name: 30, dtype: float64\n"]}],"source":["df_sorted = random_forest_experiment(method=None)\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["#### Class Weight Adjustment"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>parameter</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>144</th>\n","      <td>10.0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>balanced</td>\n","      <td>0.594315</td>\n","      <td>0.845750</td>\n","    </tr>\n","    <tr>\n","      <th>122</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>balanced</td>\n","      <td>0.591093</td>\n","      <td>0.839328</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>balanced</td>\n","      <td>0.588563</td>\n","      <td>0.834270</td>\n","    </tr>\n","    <tr>\n","      <th>104</th>\n","      <td>10.0</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>balanced</td>\n","      <td>0.585608</td>\n","      <td>0.845286</td>\n","    </tr>\n","    <tr>\n","      <th>150</th>\n","      <td>10.0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>balanced</td>\n","      <td>0.585242</td>\n","      <td>0.842479</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     max_depth  min_samples_split  min_samples_leaf parameter  f1_score  \\\n","144       10.0                  2                 1  balanced  0.594315   \n","122        NaN                 10                 2  balanced  0.591093   \n","66         NaN                 10                 1  balanced  0.588563   \n","104       10.0                 10                 2  balanced  0.585608   \n","150       10.0                  5                 1  balanced  0.585242   \n","\n","      auc_roc  \n","144  0.845750  \n","122  0.839328  \n","66   0.834270  \n","104  0.845286  \n","150  0.842479  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth                10.0\n","min_samples_split           2\n","min_samples_leaf            1\n","parameter            balanced\n","f1_score             0.594315\n","auc_roc               0.84575\n","Name: 144, dtype: object\n"]}],"source":["df_sorted = random_forest_experiment(method='class_weight')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'parameter', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["### Upsampling"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    7963\n","1    7963\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>54</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.9965</td>\n","      <td>0.990704</td>\n","      <td>0.997031</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.9965</td>\n","      <td>0.990704</td>\n","      <td>0.997058</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.9940</td>\n","      <td>0.984000</td>\n","      <td>0.996637</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.9935</td>\n","      <td>0.982690</td>\n","      <td>0.996339</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.9905</td>\n","      <td>0.974834</td>\n","      <td>0.996339</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    max_depth  min_samples_split  min_samples_leaf  accuracy  f1_score  \\\n","54        NaN                  2                 1    0.9965  0.990704   \n","27        NaN                  2                 1    0.9965  0.990704   \n","30        NaN                  5                 1    0.9940  0.984000   \n","57        NaN                  5                 1    0.9935  0.982690   \n","0         NaN                  2                 1    0.9905  0.974834   \n","\n","     auc_roc  \n","54  0.997031  \n","27  0.997058  \n","30  0.996637  \n","57  0.996339  \n","0   0.996339  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth                 NaN\n","min_samples_split    2.000000\n","min_samples_leaf     1.000000\n","accuracy             0.996500\n","f1_score             0.990704\n","auc_roc              0.997031\n","Name: 54, dtype: float64\n"]}],"source":["df_sorted = random_forest_experiment(method='upsampling')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'accuracy', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["### Downsampling"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    2037\n","1    2037\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>accuracy</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>54</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.8775</td>\n","      <td>0.756219</td>\n","      <td>0.987168</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.8710</td>\n","      <td>0.746562</td>\n","      <td>0.985928</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.8710</td>\n","      <td>0.742515</td>\n","      <td>0.970873</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.8645</td>\n","      <td>0.734053</td>\n","      <td>0.972680</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.8630</td>\n","      <td>0.729783</td>\n","      <td>0.967571</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    max_depth  min_samples_split  min_samples_leaf  accuracy  f1_score  \\\n","54        NaN                  2                 1    0.8775  0.756219   \n","27        NaN                  2                 1    0.8710  0.746562   \n","0         NaN                  2                 1    0.8710  0.742515   \n","57        NaN                  5                 1    0.8645  0.734053   \n","55        NaN                  2                 2    0.8630  0.729783   \n","\n","     auc_roc  \n","54  0.987168  \n","27  0.985928  \n","0   0.970873  \n","57  0.972680  \n","55  0.967571  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth                 NaN\n","min_samples_split    2.000000\n","min_samples_leaf     1.000000\n","accuracy             0.877500\n","f1_score             0.756219\n","auc_roc              0.987168\n","Name: 54, dtype: float64\n"]}],"source":["df_sorted = random_forest_experiment(method='downsampling')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'accuracy', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["### Threshold Adjustment"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exited\n","0    4773\n","1    1227\n","Name: count, dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>max_depth</th>\n","      <th>min_samples_split</th>\n","      <th>min_samples_leaf</th>\n","      <th>parameter</th>\n","      <th>f1_score</th>\n","      <th>auc_roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>668</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0.40</td>\n","      <td>0.604046</td>\n","      <td>0.835946</td>\n","    </tr>\n","    <tr>\n","      <th>1207</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0.35</td>\n","      <td>0.600791</td>\n","      <td>0.840374</td>\n","    </tr>\n","    <tr>\n","      <th>667</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0.35</td>\n","      <td>0.597610</td>\n","      <td>0.835946</td>\n","    </tr>\n","    <tr>\n","      <th>707</th>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>0.35</td>\n","      <td>0.593923</td>\n","      <td>0.838596</td>\n","    </tr>\n","    <tr>\n","      <th>587</th>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>0.35</td>\n","      <td>0.593923</td>\n","      <td>0.838596</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      max_depth  min_samples_split  min_samples_leaf  parameter  f1_score  \\\n","668         NaN                 10                 1       0.40  0.604046   \n","1207        NaN                 10                 1       0.35  0.600791   \n","667         NaN                 10                 1       0.35  0.597610   \n","707         NaN                 10                 5       0.35  0.593923   \n","587         NaN                  2                 5       0.35  0.593923   \n","\n","       auc_roc  \n","668   0.835946  \n","1207  0.840374  \n","667   0.835946  \n","707   0.838596  \n","587   0.838596  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["max_depth                  NaN\n","min_samples_split    10.000000\n","min_samples_leaf      1.000000\n","parameter             0.400000\n","f1_score              0.604046\n","auc_roc               0.835946\n","Name: 668, dtype: float64\n"]}],"source":["df_sorted = random_forest_experiment(method='threshold')\n","df_display = df_sorted[['max_depth', 'min_samples_split', 'min_samples_leaf', 'parameter', 'f1_score', 'auc_roc']]\n","display(df_display.head())\n","if df_sorted.iloc[0]['f1_score'] > best['f1_score']:\n","    best = df_sorted.iloc[0]\n","print('Best so far:', best)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\">\n","<b>Reviewer's comment</b>\n","\n","Upampling should be applied only to the train set, otherwise it won't be possible to accurately estimate how the model will generalize to new data for two reasons:\n","    \n","1. Validation/test data obtained from an upsampled full dataset will not have the same distribution as actual data (which is not balanced)\n","2. There are bound to be the same examples in train and test, which is a clear case of data leakage.\n","    \n","The goal of upsampling is just to help the model better learn about the underrepresented class, but the validation and test set need to have the original data distribution in order for evaluation to make any sense.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\">\n","<b>Reviewer's comment</b>\n","\n","The same comment as for upsampling: downsampling should only be applied to the train set. While the argument about having the same examples in train and test no longer works, the first point about validation/test data needing to have the original data distribution for accurate estimation of generalization performance applies here.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\">\n","<b>Reviewer's comment</b>\n","\n","Please check the results after making sure that the test set is only used for final model evaluation and all prior comparisons are done using the validation set \n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion\n","\n","This project involved building a machine learning model to predict customer churn. The dataset was initially imbalanced with a larger number of customers who continued their business with Beta Bank compared to those who did not. The initial model which was trained without addressing the imbalance performed poorly having a low F1 score for the minority class. After addressing the class imbalance using both upsampling and downsampling, the F1 scores improved dramatically from .12 to around .63-.64. The AUC-ROC scores of the improved model was around .65-.69.\n","\n","This project demonstrated the importance of properly preprocessing the data, handling class imbalance, and choosing the right evaluation metrics when working with imbalanced datasets. It also shows the iterative process of building a model and continually improving the model based on performance."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\">\n","<b>Reviewer's comment</b>\n","\n","Don't forget to change the conclusions if needed\n","\n","</div>"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":2}
